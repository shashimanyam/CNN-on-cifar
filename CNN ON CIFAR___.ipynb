{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "wVIx_KIigxPV",
    "outputId": "7e02c581-5052-4c66-9f9b-33e862a3f5a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot\n",
    "from prettytable import PrettyTable\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler,CSVLogger, Callback,ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "984hP2OauLCe",
    "outputId": "cfcf2ef2-cc1a-4ddf-f5c9-86c937a7e2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "# import tensorflow as tf\n",
    "# tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
    "# tf.config.gpu.set_per_process_memory_growth(True)\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "# k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2U6NU_27K7X"
   },
   "outputs": [],
   "source": [
    "final_tab = PrettyTable(['Augmentation','l','num_filters','compression','Optimizer','Test Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "l = 9\n",
    "num_filter = 24\n",
    "compression = 1.041\n",
    "dropout_rate = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "c127adfa-b248-474b-aad6-7220650689a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "O8jnOucc1dRO",
    "outputId": "3a881ee6-a29c-4b86-d6ab-c7e86e35accb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (50000, 32, 32, 3)\n",
      "Test Shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train Shape:',X_train.shape)\n",
    "print('Test Shape:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    print('input',input.shape)\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    print('Batch',BatchNorm.shape)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    print('relu',relu.shape)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    # print('pooling',AvgPooling.shape)\n",
    "    # flat = layers.Flatten()(AvgPooling)\n",
    "    # print('flat',flat.shape)\n",
    "    # # tf.reshape(flat,(4,246))\n",
    "    # # print(flat.reshape(7,4,4,246,1))\n",
    "    # output = layers.Conv1D(num_filter,kernel_size=1)(tf.reshape(flat,(1,4,246)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    conv_layer = layers.Conv2D(10, (1,1), use_bias=False ,padding='same')(AvgPooling)\n",
    "    last = layers.GlobalMaxPooling2D()(conv_layer)\n",
    "    output = layers.Activation('softmax')(last)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWG51i8_zIr-"
   },
   "outputs": [],
   "source": [
    "num_filter = 12\n",
    "dropout_rate = 0.2\n",
    "l = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "anPCpQWhhGb7",
    "outputId": "0b799eb1-232a-4fb5-a9ef-a7d26779d6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "input (?, 4, 4, 240)\n",
      "Batch (?, 4, 4, 240)\n",
      "relu (?, 4, 4, 240)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGpbWr5J1dRf"
   },
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1608.06993.pdf\n",
    "from IPython.display import IFrame, YouTubeVideo\n",
    "YouTubeVideo(id='-W6y8xnd--U', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "59f59e4c-eeba-491e-b5e2-0b26efe029e0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 24)   648         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 24)   96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 24)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 24)   5184        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 32, 24)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 48)   0           conv2d[0][0]                     \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 48)   192         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 48)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 24)   10368       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 24)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 72)   0           concatenate[0][0]                \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 72)   288         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 72)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 24)   15552       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 24)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 96)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 96)   384         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 96)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 24)   20736       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 24)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 120)  0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 120)  480         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 120)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 24)   25920       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 24)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 144)  0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 144)  576         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 144)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 24)   31104       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 24)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 168)  0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 168)  672         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 168)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 24)   36288       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 24)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 192)  0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 192)  768         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 192)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 24)   41472       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 24)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 216)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 216)  864         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 216)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 24)   46656       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 24)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 240)  0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 240)  960         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 240)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 24)   5760        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 24)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 24)   0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 24)   96          average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 24)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 24)   5184        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 16, 24)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 48)   0           average_pooling2d[0][0]          \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 48)   192         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 48)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 24)   10368       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 16, 24)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 72)   0           concatenate_9[0][0]              \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 72)   288         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 72)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 24)   15552       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 24)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 96)   0           concatenate_10[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 96)   384         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 96)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 24)   20736       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16, 16, 24)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 120)  0           concatenate_11[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 120)  480         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 120)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 24)   25920       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 24)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 144)  0           concatenate_12[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 144)  576         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 144)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 24)   31104       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 16, 24)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 168)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 168)  672         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 168)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 24)   36288       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 24)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 192)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 192)  768         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 192)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 24)   41472       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 24)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 216)  0           concatenate_15[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 216)  864         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 216)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 24)   46656       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 24)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 240)  0           concatenate_16[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 240)  960         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 240)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 24)   5760        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 24)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 24)     0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 24)     96          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 24)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 24)     5184        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 8, 8, 24)     0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 48)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 48)     192         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 48)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 24)     10368       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 8, 8, 24)     0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 8, 8, 72)     0           concatenate_18[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 72)     288         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 72)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 24)     15552       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 8, 24)     0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 96)     0           concatenate_19[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 96)     384         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 96)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 24)     20736       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 8, 8, 24)     0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 120)    0           concatenate_20[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 120)    480         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 120)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 24)     25920       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 8, 8, 24)     0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 144)    0           concatenate_21[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 144)    576         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 144)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 24)     31104       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 8, 8, 24)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 168)    0           concatenate_22[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 168)    672         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 168)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 24)     36288       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 8, 8, 24)     0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 192)    0           concatenate_23[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 192)    768         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 192)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 24)     41472       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 8, 8, 24)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 216)    0           concatenate_24[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 216)    864         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 216)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 24)     46656       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 8, 8, 24)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 240)    0           concatenate_25[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 240)    960         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 240)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 24)     5760        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 8, 8, 24)     0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 24)     0           dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 24)     96          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 24)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 24)     5184        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 4, 4, 24)     0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 4, 4, 48)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 48)     192         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 48)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 24)     10368       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 4, 4, 24)     0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 4, 4, 72)     0           concatenate_27[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 72)     288         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 72)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 24)     15552       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 4, 4, 24)     0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 4, 4, 96)     0           concatenate_28[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 96)     384         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 96)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 24)     20736       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 4, 4, 24)     0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 4, 4, 120)    0           concatenate_29[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 120)    480         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 120)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 24)     25920       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 4, 4, 24)     0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 4, 4, 144)    0           concatenate_30[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 144)    576         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 144)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 24)     31104       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 4, 4, 24)     0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 4, 4, 168)    0           concatenate_31[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 4, 4, 168)    672         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 168)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 24)     36288       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 4, 4, 24)     0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 4, 4, 192)    0           concatenate_32[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 4, 4, 192)    768         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 192)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 4, 24)     41472       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 4, 4, 24)     0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 4, 4, 216)    0           concatenate_33[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 4, 4, 216)    864         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 216)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 4, 24)     46656       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 4, 4, 24)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 4, 4, 240)    0           concatenate_34[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 4, 4, 240)    960         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 240)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 2, 2, 240)    0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 2, 2, 10)     2400        average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d (GlobalMax (None, 10)           0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 10)           0           global_max_pooling2d[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 974,568\n",
      "Trainable params: 964,008\n",
      "Non-trainable params: 10,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "crhGk7kEhXAz",
    "outputId": "45b9e70a-f6e0-4080-da71-560b5ed4e0df",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 112s 2ms/sample - loss: 1.7266 - acc: 0.3533 - val_loss: 1.5696 - val_acc: 0.4313\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.3710 - acc: 0.4956 - val_loss: 1.6232 - val_acc: 0.4712\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.2011 - acc: 0.5626 - val_loss: 1.2632 - val_acc: 0.5566\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0969 - acc: 0.6025 - val_loss: 1.2591 - val_acc: 0.5634\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 95s 2ms/sample - loss: 1.0390 - acc: 0.6257 - val_loss: 1.2728 - val_acc: 0.5842\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9857 - acc: 0.6452 - val_loss: 1.4487 - val_acc: 0.5634\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9501 - acc: 0.6597 - val_loss: 1.9306 - val_acc: 0.4998\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9204 - acc: 0.6683 - val_loss: 1.0015 - val_acc: 0.6638\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.8858 - acc: 0.6805 - val_loss: 1.3980 - val_acc: 0.5892\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.8614 - acc: 0.6891 - val_loss: 1.3597 - val_acc: 0.5937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f368f86a518>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "6df3f972-f809-44b5-faa8-c25ea1afdcc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 848us/sample - loss: 1.3597 - acc: 0.5937\n",
      "Test loss: 1.3597363298416139\n",
      "Test accuracy: 0.5937\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE3lF6EH1r_L"
   },
   "outputs": [],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1NXStrB8AcL"
   },
   "outputs": [],
   "source": [
    "# ['Augmentation','l','num_filters','compression','Optimizer','Test Accuracy']\n",
    "\n",
    "final_tab.add_row([None,l, num_filter, compression,'Adam',0.59])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "MdHgBd1p9Ib5",
    "outputId": "009e72de-f1f8-4744-8dd6-2afb61e61329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------------+-------------+-----------+---------------+\n",
      "| Augmentation | l  | num_filters | compression | Optimizer | Test Accuracy |\n",
      "+--------------+----+-------------+-------------+-----------+---------------+\n",
      "|     None     | 12 |      12     |     0.5     |    Adam   |      0.59     |\n",
      "+--------------+----+-------------+-------------+-----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(final_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEGSwJFMXObb"
   },
   "source": [
    "# DenseNet Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb4fp1L6Xiyx"
   },
   "outputs": [],
   "source": [
    "def dense_net(xtrain,xtest, optim = Adam(),k_size=(3,3), b_size = batch_size, epoch = epochs):\n",
    "          print('b_size:{} epochs:{}'.format(b_size,epoch))\n",
    "          input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "          First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "          First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "          First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "          Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "          Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "          Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "          Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "          Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "          output = output_layer(Last_Block)\n",
    "\n",
    "\n",
    "          model = Model(inputs=[input], outputs=[output])\n",
    "         \n",
    "          model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "          model.fit(xtrain, y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=epochs,\n",
    "                              verbose=1, \n",
    "                              validation_data=(xtest, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          score = model.evaluate(xtest, y_test, verbose=1)\n",
    "          print('Test loss:', score[0])\n",
    "          print('Test accuracy:', score[1])\n",
    "\n",
    "          return model\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fLA3SqPZKQz"
   },
   "source": [
    "## Image Augmentation Techniques\n",
    "`Some of the augmentation techniques are as follows`\n",
    "<ol>\n",
    "<li>Vertical Shift Augmentation</li>\n",
    "<li>Horizontal Shift Augmentation</li>\n",
    "<li>Vertical Flip Augmentation</li>\n",
    "<li>Horizontal Flip Augmentation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkzL7VAtaCEH"
   },
   "source": [
    "## Vertical and Horizontal Shift Augmentation:\n",
    "`A shift to an image means moving all pixels of the image in one direction, vertically,horizontally while keeping the image dimensions the same.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hx9ImbHRuolR"
   },
   "outputs": [],
   "source": [
    "# Reff https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "\n",
    "def vertical_horizontal_shift(arr_imgs):\n",
    "\n",
    "      # convert to numpy array\n",
    "      d = arr_imgs.copy()\n",
    "      \n",
    "      for i in tqdm(range(d.shape[0]), position=0):\n",
    "          data = d[i]\n",
    "          # expand dimension to one sample\n",
    "          samples = expand_dims(data, 0)\n",
    "          # create image data augmentation generator\n",
    "          datagen = ImageDataGenerator(width_shift_range=[-15,15], height_shift_range=[-15,15])\n",
    "          # prepare iterator\n",
    "          it = datagen.flow(samples, batch_size=1)\n",
    "          # generate samples and plot\n",
    "          # define subplot\n",
    "          # pyplot.subplot(330 + 1 + i)\n",
    "          # generate batch of images\n",
    "          for j in range(9):\n",
    "              batch = it.next()     \n",
    "              if j == 0:\n",
    "\n",
    "                  # convert to unsigned integers for viewing\n",
    "                  image = batch[0].astype('uint8')\n",
    "                  d[i] = image\n",
    "                  # plot raw pixel data\n",
    "                  break\n",
    "      return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bH8hpC65q2p"
   },
   "source": [
    "### Original Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "7cSu54lQ5hxl",
    "outputId": "1e495ecb-0b94-4b48-a454-785db5ecf396"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fecfc932240>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXJmDkai54fZ"
   },
   "source": [
    "### After Vertical and Horizontal Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "Vse1a-Vg53tL",
    "outputId": "cd4aa02a-e730-4fb1-e2d0-292fe407c473"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10000/10000 [00:58<00:00, 169.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe0a7a5a278>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niVL4RRKl8IskSuEXSdT/AS+BApyB3idiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(vertical_horizontal_shift(X_test)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zhJ8Q3f6gI2"
   },
   "source": [
    "## Applying vertical and horizontal shift on vertical and horizontal shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FAau4lVB31aX",
    "outputId": "10792969-0ce9-4f79-96e5-55fbc8c4ff7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [00:42<00:00, 1170.97it/s]\n",
      "100%|| 10000/10000 [00:08<00:00, 1207.90it/s]\n"
     ]
    }
   ],
   "source": [
    "v_h_shift_train = vertical_horizontal_shift(X_train)\n",
    "v_h_shift_test  = vertical_horizontal_shift(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Zkqh6fPpJ6XO",
    "outputId": "77db44ce-ac4b-4d8b-c43f-0250d787f6d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe0a43b8550>"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX4hEkfMLkSj/H+uZJYrl/swKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(X_test[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "zRjA5my5JpWr",
    "outputId": "859d3ce0-db3f-48bf-cd5a-74c9e3d04d26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe0a33e3438>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(v_h_shift_test[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNtVqMdFRViw"
   },
   "source": [
    "### DenseNet with Adam Optimizer on  Vertical Horizantal Shift Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "I2Yb1OJQGD9g",
    "outputId": "76011b78-f509-4b51-fab2-bc38c74e7eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_size:64 epochs:10\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 191s 4ms/sample - loss: 2.0386 - acc: 0.2355 - val_loss: 1.9912 - val_acc: 0.2963\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.8141 - acc: 0.3226 - val_loss: 1.7621 - val_acc: 0.3535\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.6705 - acc: 0.3824 - val_loss: 1.9284 - val_acc: 0.3487\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.5842 - acc: 0.4157 - val_loss: 1.6201 - val_acc: 0.4319\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.5244 - acc: 0.4409 - val_loss: 1.7455 - val_acc: 0.4045\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.4734 - acc: 0.4619 - val_loss: 1.5040 - val_acc: 0.4666\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.4372 - acc: 0.4787 - val_loss: 1.6663 - val_acc: 0.4400\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.4007 - acc: 0.4926 - val_loss: 1.6158 - val_acc: 0.4545\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.3735 - acc: 0.5033 - val_loss: 1.4835 - val_acc: 0.4785\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 174s 3ms/sample - loss: 1.3441 - acc: 0.5166 - val_loss: 1.5579 - val_acc: 0.4632\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 1.5579 - acc: 0.4632\n",
      "Test loss: 1.5579216079711915\n",
      "Test accuracy: 0.4632\n"
     ]
    }
   ],
   "source": [
    "v_h_shift_model = dense_net(v_h_shift_train, v_h_shift_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q07A6xHf9O3a"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_Shift',l, num_filter, compression,'Adam',0.42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTcPMXeyRkek"
   },
   "source": [
    "### DenseNet with Nadam Optimizer on  Vertical Horizantal Shift Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "Xnkr9dfBBe6C",
    "outputId": "d43d6cae-81f0-4f49-8da7-43f085b658b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 105s 2ms/sample - loss: 2.1528 - acc: 0.1884 - val_loss: 2.3496 - val_acc: 0.1900\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 95s 2ms/sample - loss: 2.0143 - acc: 0.2405 - val_loss: 2.0252 - val_acc: 0.2460\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.8841 - acc: 0.2924 - val_loss: 2.0804 - val_acc: 0.2567\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.7920 - acc: 0.3263 - val_loss: 1.9178 - val_acc: 0.3146\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.7242 - acc: 0.3547 - val_loss: 1.7486 - val_acc: 0.3612\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.6726 - acc: 0.3753 - val_loss: 1.7488 - val_acc: 0.3691\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.6298 - acc: 0.3925 - val_loss: 1.7267 - val_acc: 0.3831\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.5924 - acc: 0.4099 - val_loss: 1.7654 - val_acc: 0.3777\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.5621 - acc: 0.4232 - val_loss: 1.6809 - val_acc: 0.4004\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.5330 - acc: 0.4333 - val_loss: 1.5949 - val_acc: 0.4122\n",
      "10000/10000 [==============================] - 8s 757us/sample - loss: 1.5949 - acc: 0.4122\n",
      "Test loss: 1.5949444211959838\n",
      "Test accuracy: 0.4122\n"
     ]
    }
   ],
   "source": [
    "v_h_shift_model_nadam = dense_net(v_h_shift_train, v_h_shift_test, optim=Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TJuB50hIfkV"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_Shift',l, num_filter, compression,'Nadam',0.41])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICeWlG3qAot8"
   },
   "source": [
    "# Horizontal and Vertical Flip Augmentation\n",
    "\n",
    "`An image flip means reversing the rows or columns of pixels in the case of a vertical or horizontal flip respectively.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPeV2xzkA9OV"
   },
   "outputs": [],
   "source": [
    "# Reff https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "\n",
    "\n",
    "def vertical_horizontal_flip(arr_imgs):\n",
    "\n",
    "      # convert to numpy array\n",
    "      d = arr_imgs.copy()\n",
    "      \n",
    "      for i in tqdm(range(d.shape[0])):\n",
    "          data = d[i]\n",
    "          # expand dimension to one sample\n",
    "          samples = expand_dims(data, 0)\n",
    "          # create image data augmentation generator\n",
    "          datagen = ImageDataGenerator(vertical_flip=True, horizontal_flip=True)\n",
    "          # prepare iterator\n",
    "          it = datagen.flow(samples, batch_size=1)\n",
    "          # generate samples and plot\n",
    "          # define subplot\n",
    "          # pyplot.subplot(330 + 1 + i)\n",
    "          # generate batch of images\n",
    "          for j in range(9):\n",
    "            batch = it.next()\n",
    "            if j == 2:\n",
    "              # convert to unsigned integers for viewing\n",
    "              image = batch[0].astype('uint8')\n",
    "              d[i] = image\n",
    "              break\n",
    "            # plot raw pixel data\n",
    "      return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KS9mBbNzVffY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpeQ9sYLSQ34"
   },
   "source": [
    "### DenseNet with Optimizer on  Vertical Horizantal Flip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yS1cElhhBey9",
    "outputId": "1fc489d8-d38c-432b-959c-137a47331ac0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [00:25<00:00, 1929.50it/s]\n",
      "100%|| 10000/10000 [00:05<00:00, 1898.82it/s]\n"
     ]
    }
   ],
   "source": [
    "v_h_flip_xtrain = vertical_horizontal_flip(X_train)\n",
    "v_h_flip_xtest  = vertical_horizontal_flip(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov-4CBNMS1Dg"
   },
   "source": [
    "#### Before Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "iI2X3pAkSTjn",
    "outputId": "f8080b94-dc41-4e7a-8441-b5fa38add8e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe0530cf400>"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kywt6iXVS36A"
   },
   "source": [
    "#### After Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "CGFS-18lSZiM",
    "outputId": "f9bb7292-6356-4d4b-a76f-a2da7c1369c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe053061550>"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(v_h_flip_xtrain[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "4s2uJQmqNBBA",
    "outputId": "4c809804-2093-40b5-f60c-0d065fca56aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 107s 2ms/sample - loss: 1.7911 - acc: 0.3097 - val_loss: 1.6606 - val_acc: 0.3809\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.5231 - acc: 0.4242 - val_loss: 1.6694 - val_acc: 0.4135\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.3828 - acc: 0.4865 - val_loss: 1.4306 - val_acc: 0.4768\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.2782 - acc: 0.5326 - val_loss: 1.4148 - val_acc: 0.5241\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1989 - acc: 0.5606 - val_loss: 1.3927 - val_acc: 0.5238\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1467 - acc: 0.5804 - val_loss: 1.4058 - val_acc: 0.5260\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1055 - acc: 0.5982 - val_loss: 1.7325 - val_acc: 0.4948\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0689 - acc: 0.6086 - val_loss: 1.3379 - val_acc: 0.5305\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0425 - acc: 0.6206 - val_loss: 1.4536 - val_acc: 0.5383\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0203 - acc: 0.6314 - val_loss: 1.3940 - val_acc: 0.5526\n",
      "10000/10000 [==============================] - 8s 824us/sample - loss: 1.3940 - acc: 0.5526\n",
      "Test loss: 1.3939515771865845\n",
      "Test accuracy: 0.5526\n"
     ]
    }
   ],
   "source": [
    "v_h_flip_model = dense_net(v_h_flip_xtrain, v_h_flip_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-GtMshXd_ck"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_Flip',l, num_filter, compression,'Adam',0.55])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpbJS8BwXEjH"
   },
   "source": [
    "### DenseNet with Nadam Optimizer on  Vertical Horizantal Flip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "_xj9ZlRRNA76",
    "outputId": "dff2d39a-1395-4a70-b9a6-cc87e372be36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 108s 2ms/sample - loss: 1.8078 - acc: 0.3088 - val_loss: 1.7010 - val_acc: 0.3601\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.5287 - acc: 0.4237 - val_loss: 1.4924 - val_acc: 0.4579\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.4101 - acc: 0.4767 - val_loss: 1.5825 - val_acc: 0.4363\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.3354 - acc: 0.5046 - val_loss: 1.4079 - val_acc: 0.4948\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.2751 - acc: 0.5342 - val_loss: 1.5963 - val_acc: 0.4315\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.2214 - acc: 0.5540 - val_loss: 1.3639 - val_acc: 0.5135\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1718 - acc: 0.5729 - val_loss: 1.3339 - val_acc: 0.5423\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1302 - acc: 0.5892 - val_loss: 1.2992 - val_acc: 0.5498\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1001 - acc: 0.6005 - val_loss: 1.2659 - val_acc: 0.5631\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0706 - acc: 0.6119 - val_loss: 1.1701 - val_acc: 0.5946\n",
      "10000/10000 [==============================] - 8s 823us/sample - loss: 1.1701 - acc: 0.5946\n",
      "Test loss: 1.1700947647094726\n",
      "Test accuracy: 0.5946\n"
     ]
    }
   ],
   "source": [
    "v_h_flip_model_nadam = dense_net(v_h_flip_xtrain, v_h_flip_xtest, optim = Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEYShehajAEA"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_Flip',l, num_filter, compression,'Nadam',0.59])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BgFIK3BbrWv"
   },
   "source": [
    "## Brightness Augmentation\n",
    "`The brightness of the image can be augmented by either randomly darkening images, brightening images, or both.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYbPcfFRNA4I"
   },
   "outputs": [],
   "source": [
    "def brightness(arr_imgs):\n",
    "\n",
    "      # convert to numpy array\n",
    "      d = arr_imgs.copy()\n",
    "      \n",
    "      for i in tqdm(range(d.shape[0])):\n",
    "          data = d[i]\n",
    "          # expand dimension to one sample\n",
    "          samples = expand_dims(data, 0)\n",
    "          # create image data augmentation generator\n",
    "          datagen = ImageDataGenerator(brightness_range=[0.5,0.6])\n",
    "          # prepare iterator\n",
    "          it = datagen.flow(samples, batch_size=1)\n",
    "          # generate samples and plot\n",
    "          # define subplot\n",
    "          # pyplot.subplot(330 + 1 + i)\n",
    "          # generate batch of images\n",
    "          for j in range(9):\n",
    "            batch = it.next()\n",
    "            if j == 8:\n",
    "              # convert to unsigned integers for viewing\n",
    "              image = batch[0].astype('uint8')\n",
    "              d[i] = image\n",
    "              break\n",
    "            # plot raw pixel data\n",
    "      return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "F9A6gLUWNAy0",
    "outputId": "cdf122b2-6737-4cf4-e028-c07c096bceda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [02:29<00:00, 333.62it/s]\n",
      "100%|| 10000/10000 [00:30<00:00, 328.29it/s]\n"
     ]
    }
   ],
   "source": [
    "bright_xtrain = brightness(X_train)\n",
    "bright_xtest = brightness(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "bYchQpVpgORm",
    "outputId": "296b5393-97e0-485c-c396-1c83c99ab606"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe04e823518>"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "fEIATdskgVJw",
    "outputId": "d965ad57-18e9-4a07-9f75-794e67e2c499"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe04dbf18d0>"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkJKVjaEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(bright_xtrain[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLC6Et9nhXLf"
   },
   "source": [
    "### DenseNet with Adam Optimizer on Brightness Augmentation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "O5j-XB6XccZ6",
    "outputId": "20e8c29b-dbcc-4ee5-8312-380d6154a347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 110s 2ms/sample - loss: 1.7495 - acc: 0.3368 - val_loss: 2.0044 - val_acc: 0.3110\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 1.3959 - acc: 0.4854 - val_loss: 1.5577 - val_acc: 0.4708\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 1.2084 - acc: 0.5608 - val_loss: 1.2202 - val_acc: 0.5645\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0993 - acc: 0.6017 - val_loss: 1.3322 - val_acc: 0.5412\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0283 - acc: 0.6269 - val_loss: 1.1301 - val_acc: 0.6099\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.9844 - acc: 0.6458 - val_loss: 1.2942 - val_acc: 0.5640\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9409 - acc: 0.6628 - val_loss: 1.1198 - val_acc: 0.6105\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.9053 - acc: 0.6769 - val_loss: 1.5355 - val_acc: 0.5591\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.8834 - acc: 0.6845 - val_loss: 1.2051 - val_acc: 0.6246\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.8541 - acc: 0.6950 - val_loss: 0.9743 - val_acc: 0.6735\n",
      "10000/10000 [==============================] - 8s 782us/sample - loss: 0.9743 - acc: 0.6735\n",
      "Test loss: 0.9742989232063294\n",
      "Test accuracy: 0.6735\n"
     ]
    }
   ],
   "source": [
    "bright_model = dense_net(bright_xtrain, bright_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIWyuLKvnJ-i"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Brightness',l, num_filter, compression,'Adam',0.67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6dkVkiFhqa7"
   },
   "source": [
    "### DenseNet with Nadam Optimizer on Brightness Augmentation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "SBc2hm9occC6",
    "outputId": "b5e8efd2-8b37-4f95-cd88-52fc6aa10916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 112s 2ms/sample - loss: 1.7422 - acc: 0.3393 - val_loss: 1.6212 - val_acc: 0.4032\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 1.3962 - acc: 0.4847 - val_loss: 1.5139 - val_acc: 0.4873\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 1.2154 - acc: 0.5559 - val_loss: 1.1808 - val_acc: 0.5713\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.1144 - acc: 0.5982 - val_loss: 1.2420 - val_acc: 0.5821\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.0450 - acc: 0.6246 - val_loss: 1.0905 - val_acc: 0.6186\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9883 - acc: 0.6453 - val_loss: 1.2437 - val_acc: 0.5991\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9478 - acc: 0.6600 - val_loss: 1.3254 - val_acc: 0.5601\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.9093 - acc: 0.6746 - val_loss: 1.1406 - val_acc: 0.6206\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.8846 - acc: 0.6830 - val_loss: 0.9157 - val_acc: 0.6844\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 0.8584 - acc: 0.6917 - val_loss: 0.9772 - val_acc: 0.6706\n",
      "10000/10000 [==============================] - 9s 924us/sample - loss: 0.9772 - acc: 0.6706\n",
      "Test loss: 0.9772336851119995\n",
      "Test accuracy: 0.6706\n"
     ]
    }
   ],
   "source": [
    "bright_model_nadam = dense_net(bright_xtrain, bright_xtest, optim=Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auaraTQQnWbd"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Brightness',l, num_filter, compression,'Nadam',0.67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTrAKbFfsyBV"
   },
   "source": [
    "## Feature Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMHzMip_nQHz"
   },
   "outputs": [],
   "source": [
    "def standard(arr_imgs):\n",
    "\n",
    "      # convert to numpy array\n",
    "      d = arr_imgs.copy()\n",
    "      \n",
    "      for i in tqdm(range(d.shape[0])):\n",
    "          data = d[i]\n",
    "          # expand dimension to one sample\n",
    "          samples = expand_dims(data, 0)\n",
    "          # create image data augmentation generator\n",
    "          datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "          # prepare iterator\n",
    "          it = datagen.flow(samples, batch_size=1)\n",
    "          # generate samples and plot\n",
    "          # define subplot\n",
    "          # pyplot.subplot(330 + 1 + i)\n",
    "          # generate batch of images\n",
    "          for j in range(9):\n",
    "              batch = it.next()\n",
    "              if j == 5:\n",
    "                # convert to unsigned integers for viewing\n",
    "                image = batch[0].astype('uint8')\n",
    "                d[i] = image\n",
    "                break\n",
    "                # plot raw pixel data\n",
    "      return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "U59Z5oImnQD6",
    "outputId": "eb620740-d755-4787-8c4c-a3fc094d00dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "100%|| 10000/10000 [00:08<00:00, 1211.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# stand_xtrain = standard(X_train)\n",
    "stand_xtest  = standard(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtjfDipu66je"
   },
   "source": [
    "### DenseNet with Adam Optimizer on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "aYByEpMNnP_E",
    "outputId": "b47273d8-072a-4201-99a0-6927227c2659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 116s 2ms/sample - loss: 1.7418 - acc: 0.3469 - val_loss: 1.5594 - val_acc: 0.4418\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.3799 - acc: 0.4947 - val_loss: 1.6403 - val_acc: 0.4437\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.2129 - acc: 0.5580 - val_loss: 1.1566 - val_acc: 0.5843\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 1.1102 - acc: 0.6008 - val_loss: 1.2747 - val_acc: 0.5612\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 1.0414 - acc: 0.6250 - val_loss: 1.2837 - val_acc: 0.5759\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.9909 - acc: 0.6446 - val_loss: 1.2212 - val_acc: 0.5953\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.9481 - acc: 0.6585 - val_loss: 1.1835 - val_acc: 0.6058\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.9093 - acc: 0.6741 - val_loss: 1.0955 - val_acc: 0.6406\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.8840 - acc: 0.6811 - val_loss: 0.8493 - val_acc: 0.6984\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 0.8602 - acc: 0.6903 - val_loss: 1.0805 - val_acc: 0.6348\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 1.0805 - acc: 0.6348\n",
      "Test loss: 1.0804764985084534\n",
      "Test accuracy: 0.6348\n"
     ]
    }
   ],
   "source": [
    "stand_model = dense_net(stand_xtrain, stand_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqmVBORiwN7n"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Standardized',l, num_filter, compression,'Adam',0.63])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08jwnvVT75xB"
   },
   "source": [
    "## DenseNet with Nadam Optimizer on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "5yn_9jT_7Gwv",
    "outputId": "a537d9a0-ff17-4c94-d375-304d6e83b6d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 117s 2ms/sample - loss: 1.7193 - acc: 0.3541 - val_loss: 2.2187 - val_acc: 0.3326\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.3688 - acc: 0.4973 - val_loss: 1.5857 - val_acc: 0.4589\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.2211 - acc: 0.5536 - val_loss: 1.5198 - val_acc: 0.5076\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.1158 - acc: 0.5921 - val_loss: 1.4742 - val_acc: 0.5246\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.0399 - acc: 0.6241 - val_loss: 1.0491 - val_acc: 0.6400\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.9917 - acc: 0.6427 - val_loss: 1.0171 - val_acc: 0.6433\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.9490 - acc: 0.6574 - val_loss: 1.4362 - val_acc: 0.5504\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.9109 - acc: 0.6737 - val_loss: 1.0236 - val_acc: 0.6586\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.8778 - acc: 0.6854 - val_loss: 1.4910 - val_acc: 0.5662\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 0.8548 - acc: 0.6942 - val_loss: 0.9976 - val_acc: 0.6743\n",
      "10000/10000 [==============================] - 10s 1ms/sample - loss: 0.9976 - acc: 0.6743\n",
      "Test loss: 0.9975716045379639\n",
      "Test accuracy: 0.6743\n"
     ]
    }
   ],
   "source": [
    "stand_model_nadam = dense_net(stand_xtrain,stand_xtest, optim = Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0O0neDn4IPZ"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Standardized',l, num_filter, compression,'Nadam',0.63])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnuJBO633qR8"
   },
   "source": [
    "<strong>Now lets try with changing some of the parameters</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dz7Mda7o7Grq"
   },
   "outputs": [],
   "source": [
    "l = 8\n",
    "num_filter = 38\n",
    "compression = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47mjuCSO58db"
   },
   "source": [
    "## DenseNet with Adam Optimizer on Vertical Horizantal Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "PvopMqwLkQOE",
    "outputId": "fe24862f-18b3-446f-d512-ec3ff86e840e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 295s 6ms/sample - loss: 1.9932 - acc: 0.2628 - val_loss: 2.8806 - val_acc: 0.2111\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 269s 5ms/sample - loss: 1.6862 - acc: 0.3815 - val_loss: 1.8009 - val_acc: 0.3622\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 269s 5ms/sample - loss: 1.5389 - acc: 0.4382 - val_loss: 1.7920 - val_acc: 0.4283\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 269s 5ms/sample - loss: 1.4265 - acc: 0.4839 - val_loss: 1.6365 - val_acc: 0.4426\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 269s 5ms/sample - loss: 1.3483 - acc: 0.5146 - val_loss: 1.5748 - val_acc: 0.4719\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.2871 - acc: 0.5360 - val_loss: 1.4691 - val_acc: 0.4993\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.2317 - acc: 0.5591 - val_loss: 1.5214 - val_acc: 0.4985\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 1.1819 - acc: 0.5803 - val_loss: 1.5452 - val_acc: 0.5099\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 1.1379 - acc: 0.5932 - val_loss: 1.6148 - val_acc: 0.4963\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 1.1026 - acc: 0.6052 - val_loss: 1.5244 - val_acc: 0.5223\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.5244 - acc: 0.5223\n",
      "Test loss: 1.5243553981781006\n",
      "Test accuracy: 0.5223\n"
     ]
    }
   ],
   "source": [
    "v_h_shift_model2 = dense_net(v_h_shift_train,v_h_shift_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB53d8ZK6Uj4"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_shift',l, num_filter, compression,'Adam',0.52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdiUK8diB98D"
   },
   "source": [
    "## DenseNet with Nadam Optimizer on Vertical Horizantal Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "dFsHxrEL6Ty-",
    "outputId": "4c803c02-b2ab-42fe-d2a9-b1e418aaf750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 288s 6ms/sample - loss: 1.9957 - acc: 0.2672 - val_loss: 2.0370 - val_acc: 0.2677\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.6974 - acc: 0.3788 - val_loss: 5.1156 - val_acc: 0.1736\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.5542 - acc: 0.4349 - val_loss: 1.7738 - val_acc: 0.3912\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.4505 - acc: 0.4729 - val_loss: 2.0385 - val_acc: 0.3929\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.3684 - acc: 0.5075 - val_loss: 1.6887 - val_acc: 0.4409\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.3042 - acc: 0.5300 - val_loss: 1.4545 - val_acc: 0.5053\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.2509 - acc: 0.5508 - val_loss: 1.8779 - val_acc: 0.4289\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.2035 - acc: 0.5672 - val_loss: 1.8281 - val_acc: 0.4424\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 270s 5ms/sample - loss: 1.1597 - acc: 0.5850 - val_loss: 3.2673 - val_acc: 0.3621\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 1.1192 - acc: 0.6013 - val_loss: 1.4267 - val_acc: 0.5364\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 1.4267 - acc: 0.5364\n",
      "Test loss: 1.426656289100647\n",
      "Test accuracy: 0.5364\n"
     ]
    }
   ],
   "source": [
    "v_h_shift_model2_nadam = dense_net(v_h_shift_train,v_h_shift_test,optim=Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2G6UazikQKq"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_shift',l, num_filter, compression,'Nadam',0.53])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "conES9h8COby"
   },
   "source": [
    "## DenseNet with Adam Optimizer on Vertical Horizantal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "XKCHfsMeCcKW",
    "outputId": "bdf660d4-deb7-4e75-f39c-7280a73c59d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 292s 6ms/sample - loss: 1.5497 - acc: 0.4271 - val_loss: 2.2234 - val_acc: 0.3542\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 1.1109 - acc: 0.6005 - val_loss: 1.9077 - val_acc: 0.4839\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.9230 - acc: 0.6673 - val_loss: 1.4761 - val_acc: 0.5829\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.8136 - acc: 0.7084 - val_loss: 1.4341 - val_acc: 0.5631\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.7315 - acc: 0.7382 - val_loss: 1.2017 - val_acc: 0.6418\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.6601 - acc: 0.7642 - val_loss: 1.3863 - val_acc: 0.6156\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.6100 - acc: 0.7844 - val_loss: 0.9802 - val_acc: 0.7015\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.5659 - acc: 0.7992 - val_loss: 1.0941 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.5260 - acc: 0.8134 - val_loss: 0.8848 - val_acc: 0.7286\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.4873 - acc: 0.8256 - val_loss: 1.0112 - val_acc: 0.7244\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.0112 - acc: 0.7244\n",
      "Test loss: 1.0111705540180207\n",
      "Test accuracy: 0.7244\n"
     ]
    }
   ],
   "source": [
    "v_h_flip_model2 = dense_net(v_h_flip_xtrain,v_h_flip_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDQUt4TQCb_U"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_flip',l, num_filter, compression,'Adam',0.72])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StuxVcQfC4xx"
   },
   "source": [
    "## DenseNet with Nadam Optimizer on Vertical Horizantal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "SrAv7eKFC8Zi",
    "outputId": "66a0df4a-51a0-41aa-b88b-38cf2d5ae6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 293s 6ms/sample - loss: 1.5328 - acc: 0.4368 - val_loss: 1.8540 - val_acc: 0.3984\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 1.0911 - acc: 0.6057 - val_loss: 1.1753 - val_acc: 0.6160\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.9227 - acc: 0.6685 - val_loss: 1.1697 - val_acc: 0.6228\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.8073 - acc: 0.7112 - val_loss: 2.4143 - val_acc: 0.4619\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.7319 - acc: 0.7377 - val_loss: 1.4257 - val_acc: 0.6086\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.6665 - acc: 0.7597 - val_loss: 1.1833 - val_acc: 0.6594\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.6127 - acc: 0.7821 - val_loss: 1.2028 - val_acc: 0.6588\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.5698 - acc: 0.7967 - val_loss: 0.8314 - val_acc: 0.7259\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.5264 - acc: 0.8124 - val_loss: 0.8623 - val_acc: 0.7422\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.4910 - acc: 0.8264 - val_loss: 1.1797 - val_acc: 0.6763\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 1.1797 - acc: 0.6763\n",
      "Test loss: 1.1797264897346496\n",
      "Test accuracy: 0.6763\n"
     ]
    }
   ],
   "source": [
    "v_h_flip_model2_nadam = dense_net(v_h_flip_xtrain,v_h_flip_xtest,optim=Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYBaNcYJC8U_"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Vertical_Horizantal_flip',l, num_filter, compression,'Nadam',0.67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1SRk3SFFNI8"
   },
   "source": [
    "## DenseNet with Adam Optimizer on Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "KEn217TqFUBg",
    "outputId": "7eb62419-d912-4e55-91bf-9d11b41240f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 262s 5ms/sample - loss: 1.3957 - acc: 0.4889 - val_loss: 2.1067 - val_acc: 0.4403\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.8968 - acc: 0.6821 - val_loss: 1.0364 - val_acc: 0.6581\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 246s 5ms/sample - loss: 0.7104 - acc: 0.7495 - val_loss: 0.9763 - val_acc: 0.6957\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 246s 5ms/sample - loss: 0.6112 - acc: 0.7871 - val_loss: 1.1059 - val_acc: 0.6766\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.5337 - acc: 0.8115 - val_loss: 1.3122 - val_acc: 0.6740\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.4847 - acc: 0.8298 - val_loss: 1.0951 - val_acc: 0.7134\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.4338 - acc: 0.8492 - val_loss: 0.7656 - val_acc: 0.7695\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.3972 - acc: 0.8608 - val_loss: 0.9332 - val_acc: 0.7533\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.3658 - acc: 0.8720 - val_loss: 0.8406 - val_acc: 0.7652\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.3417 - acc: 0.8804 - val_loss: 0.7648 - val_acc: 0.7923\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.7648 - acc: 0.7923\n",
      "Test loss: 0.7648081164598465\n",
      "Test accuracy: 0.7923\n"
     ]
    }
   ],
   "source": [
    "bright_model2 = dense_net(bright_xtrain, bright_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXg3eV7VFlH5"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Brightness',l, num_filter, compression,'Adam',0.79])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UYh4fu4Flu4"
   },
   "source": [
    "## DenseNet with Nadam Optimizer on Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "K4j0yShhFtbC",
    "outputId": "f525352f-18a9-4c2f-bccd-f242f6bbd6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 252s 5ms/sample - loss: 1.3535 - acc: 0.5083 - val_loss: 2.6686 - val_acc: 0.3786\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.8746 - acc: 0.6905 - val_loss: 1.4745 - val_acc: 0.5859\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.7027 - acc: 0.7521 - val_loss: 1.2347 - val_acc: 0.6524\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.6028 - acc: 0.7891 - val_loss: 0.9355 - val_acc: 0.7102\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.5348 - acc: 0.8140 - val_loss: 0.7049 - val_acc: 0.7758\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 246s 5ms/sample - loss: 0.4797 - acc: 0.8329 - val_loss: 0.8034 - val_acc: 0.7683\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 246s 5ms/sample - loss: 0.4343 - acc: 0.8484 - val_loss: 1.1576 - val_acc: 0.6972\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 246s 5ms/sample - loss: 0.4059 - acc: 0.8565 - val_loss: 0.6769 - val_acc: 0.8014\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 246s 5ms/sample - loss: 0.3660 - acc: 0.8709 - val_loss: 0.9298 - val_acc: 0.7320\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 247s 5ms/sample - loss: 0.3360 - acc: 0.8828 - val_loss: 1.2232 - val_acc: 0.7018\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 1.2232 - acc: 0.7018\n",
      "Test loss: 1.2232139734268188\n",
      "Test accuracy: 0.7018\n"
     ]
    }
   ],
   "source": [
    "bright_model2_nadam = dense_net(bright_xtrain, bright_xtest, optim=Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Qiew8mCFtXY"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Brightness',l, num_filter, compression,'Nadam',0.70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gCJagB5GYRN"
   },
   "source": [
    "## DenseNet with Adam Optimizer on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "aGz3ioKZFtSv",
    "outputId": "347a8cbd-f086-4a65-d73c-2e80607a0308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 290s 6ms/sample - loss: 1.3630 - acc: 0.5059 - val_loss: 1.8054 - val_acc: 0.4663\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.8947 - acc: 0.6818 - val_loss: 2.0782 - val_acc: 0.5325\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.7168 - acc: 0.7482 - val_loss: 1.2432 - val_acc: 0.6331\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.6117 - acc: 0.7844 - val_loss: 1.6101 - val_acc: 0.6382\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.5369 - acc: 0.8114 - val_loss: 1.1828 - val_acc: 0.6678\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.4824 - acc: 0.8326 - val_loss: 0.9018 - val_acc: 0.7418\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.4367 - acc: 0.8485 - val_loss: 1.2826 - val_acc: 0.6894\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.3977 - acc: 0.8611 - val_loss: 0.7730 - val_acc: 0.7711\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.3629 - acc: 0.8738 - val_loss: 1.0036 - val_acc: 0.7356\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.3349 - acc: 0.8831 - val_loss: 0.8019 - val_acc: 0.7713\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.8019 - acc: 0.7713\n",
      "Test loss: 0.8018832360267639\n",
      "Test accuracy: 0.7713\n"
     ]
    }
   ],
   "source": [
    "stand_model2 = dense_net(stand_xtrain,stand_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4FSvoU1FtO5"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Standardized',l, num_filter, compression,'Adam',0.77])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ai4lWW2QGp6K"
   },
   "source": [
    "## DenseNet with Nadam Optimizer on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "GygSr8NZFtKi",
    "outputId": "7e061ca2-5d1a-469f-8608-e6ae4cc61fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 291s 6ms/sample - loss: 1.3426 - acc: 0.5116 - val_loss: 1.8898 - val_acc: 0.4669\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.8734 - acc: 0.6906 - val_loss: 2.5935 - val_acc: 0.4521\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.7015 - acc: 0.7531 - val_loss: 1.2029 - val_acc: 0.6608\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 273s 5ms/sample - loss: 0.5990 - acc: 0.7908 - val_loss: 1.4715 - val_acc: 0.6416\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 273s 5ms/sample - loss: 0.5287 - acc: 0.8131 - val_loss: 0.6723 - val_acc: 0.7890\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 273s 5ms/sample - loss: 0.4771 - acc: 0.8332 - val_loss: 2.1752 - val_acc: 0.5709\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 272s 5ms/sample - loss: 0.4321 - acc: 0.8491 - val_loss: 0.9137 - val_acc: 0.7556\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.3960 - acc: 0.8619 - val_loss: 0.5650 - val_acc: 0.8256\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.3607 - acc: 0.8754 - val_loss: 0.7361 - val_acc: 0.7944\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 271s 5ms/sample - loss: 0.3387 - acc: 0.8808 - val_loss: 0.6667 - val_acc: 0.8088\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.6667 - acc: 0.8088\n",
      "Test loss: 0.6666583400726318\n",
      "Test accuracy: 0.8088\n"
     ]
    }
   ],
   "source": [
    "stand_model2_nadam = dense_net(stand_xtrain,stand_xtest, optim = Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xi8rPjuTHJGJ"
   },
   "outputs": [],
   "source": [
    "final_tab.add_row(['Standardized',l, num_filter, compression,'Nadam',0.80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "8kPheLf7C8Q1",
    "outputId": "fe7ae051-2c19-450c-85dd-d90405e0ac20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----+-------------+-------------+-----------+---------------+\n",
      "|        Augmentation       | l  | num_filters | compression | Optimizer | Test Accuracy |\n",
      "+---------------------------+----+-------------+-------------+-----------+---------------+\n",
      "|            None           | 12 |      12     |     0.5     |    Adam   |      0.59     |\n",
      "| Vertical_Horizantal_Shift | 12 |      12     |     0.5     |    Adam   |      0.41     |\n",
      "| Vertical_Horizantal_Shift | 12 |      12     |     0.5     |   Nadam   |     0.409     |\n",
      "|  Vertical_Horizantal_Flip | 12 |      12     |     0.5     |    Adam   |     0.604     |\n",
      "|  Vertical_Horizantal_Flip | 12 |      12     |     0.5     |   Nadam   |      0.61     |\n",
      "|         Brightness        | 12 |      12     |     0.5     |    Adam   |      0.66     |\n",
      "|         Brightness        | 12 |      12     |     0.5     |   Nadam   |      0.67     |\n",
      "|        Standardized       | 12 |      12     |     0.5     |    Adam   |      0.63     |\n",
      "|        Standardized       | 12 |      12     |     0.5     |   Nadam   |      0.63     |\n",
      "| Vertical_Horizantal_shift | 8  |      38     |      1      |    Adam   |      0.52     |\n",
      "| Vertical_Horizantal_shift | 8  |      38     |      1      |   Nadam   |      0.53     |\n",
      "|  Vertical_Horizantal_flip | 8  |      38     |      1      |    Adam   |      0.72     |\n",
      "|  Vertical_Horizantal_flip | 8  |      38     |      1      |   Nadam   |      0.67     |\n",
      "|         Brightness        | 8  |      38     |      1      |    Adam   |      0.79     |\n",
      "|         Brightness        | 8  |      38     |      1      |   Nadam   |      0.7      |\n",
      "|        Standardized       | 8  |      38     |      1      |    Adam   |      0.77     |\n",
      "|        Standardized       | 8  |      38     |      1      |   Nadam   |      0.8      |\n",
      "+---------------------------+----+-------------+-------------+-----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(final_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWOnAydG4iwq"
   },
   "source": [
    "### Observations:\n",
    "\n",
    "<strong>If we observe the test accuracy when l =8, no_filters = 38, compression = 1 is higher so we shall use these changed parameters</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWkowdD45woY"
   },
   "source": [
    "<strong>We shall add the image augmentation which influenced the test accuracy lot i.e brightness,standardization, flipping</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD0cQXI_YTOD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-GcEs3Lrww8N",
    "outputId": "867c12ce-16e6-47c1-cbfb-960828e8e43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 178 s, sys: 35 s, total: 213 s\n",
      "Wall time: 218 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    \n",
    "    brightness_range=[0.5,1.9],\n",
    "    featurewise_center=True, featurewise_std_normalization=True,\n",
    "    width_shift_range = 0.125,\n",
    "    horizontal_flip=True,vertical_flip=True,rotation_range=15,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "IKe2P6RA7Ihh",
    "outputId": "305172c4-8bc8-4a72-cd9e-957ca86b7c14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAD7CAYAAAAFI30bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAg4If4/MCDcS16zP34AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for X_batch, y_batch in datagen.flow(X_train[:9], y_train[:9], batch_size=9):\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        \n",
    "        plt.imshow(X_batch[i].astype('uint8'), cmap=plt.get_cmap('prism'))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bN7MmkHOoFq-"
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h57HoIyvgOjz"
   },
   "outputs": [],
   "source": [
    "def dense_net2(xtrain,xtest, optim = Adam(),k_size=(3,3), b_size = batch_size, epoch = epochs):\n",
    "          print('b_size:{} epochs:{} '.format(b_size,epoch))\n",
    "          input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "          First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "          First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "          First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "          Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "          Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "          Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "          Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "          Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "          output = output_layer(Last_Block)\n",
    "        \n",
    "\n",
    "          model = Model(inputs=[input], outputs=[output])\n",
    "\n",
    "          reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 5, min_lr = 0.000001)\n",
    "\n",
    "          # early_stop = EarlyStopping(monitor = \"val_loss\", patience = 10)\n",
    "\n",
    "          def decay_fn(epoch, lr):\n",
    "              if epoch < 50:\n",
    "                  return 0.001\n",
    "              elif epoch >= 50 and epoch < 75:\n",
    "                  return 0.0001\n",
    "              else:\n",
    "                  return 0.00001\n",
    "\n",
    "          lr_scheduler = LearningRateScheduler(decay_fn)\n",
    "\n",
    "          csv_logger = CSVLogger('training.log')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          checkpoint = ModelCheckpoint('gdrive/My Drive/cnnoncifar/models/model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5',\n",
    "                                       verbose=1, monitor='val_acc',save_best_only=True, mode='auto')  \n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "          model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "          # model.fit(xtrain, y_train,\n",
    "          #                     batch_size=batch_size,\n",
    "          #                     epochs=epochs,\n",
    "          #                     verbose=1, \n",
    "          #                     validation_data=(xtest, y_test))\n",
    "          print(model.summary())\n",
    "          model.fit_generator(\n",
    "            datagen.flow(xtrain, y_train, batch_size=b_size),\n",
    "            steps_per_epoch=(len(xtrain)/batch_size)*5,              \n",
    "            epochs=epoch,\n",
    "            verbose = 1,\n",
    "            validation_data=(xtest, y_test),callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "          score = model.evaluate(xtest, y_test, verbose=1)\n",
    "          print('Test loss:', score[0])\n",
    "          print('Test accuracy:', score[1])\n",
    "\n",
    "          return model\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfWa-vxxC652"
   },
   "outputs": [],
   "source": [
    "# l = 8\n",
    "# num_filter = 27\n",
    "compression = 1.041\n",
    "# \n",
    "# \n",
    "\n",
    "\n",
    "# no of layers in dense block\n",
    "l = 9 \n",
    "# growth rate k\n",
    "num_filter = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QPgSkHRpgSGm",
    "outputId": "92b38c06-4a2a-4143-b5a9-171c2efd5ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_size:128 epochs:150 \n",
      "input (?, 4, 4, 240)\n",
      "Batch (?, 4, 4, 240)\n",
      "relu (?, 4, 4, 240)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 32, 32, 24)   648         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 32, 32, 24)   96          conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 32, 32, 24)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 32, 32, 24)   5184        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 32, 32, 24)   0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 32, 32, 48)   0           conv2d_82[0][0]                  \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 32, 32, 48)   192         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 32, 32, 48)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 32, 32, 24)   10368       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 32, 32, 24)   0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 32, 32, 72)   0           concatenate_72[0][0]             \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 32, 32, 72)   288         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 32, 32, 72)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 32, 32, 24)   15552       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 32, 32, 24)   0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 32, 32, 96)   0           concatenate_73[0][0]             \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 32, 32, 96)   384         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 32, 32, 96)   0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 32, 32, 24)   20736       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 32, 32, 24)   0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 32, 32, 120)  0           concatenate_74[0][0]             \n",
      "                                                                 dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 32, 32, 120)  480         concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 32, 32, 120)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 24)   25920       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 32, 32, 24)   0           conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 32, 32, 144)  0           concatenate_75[0][0]             \n",
      "                                                                 dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 32, 32, 144)  576         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 32, 32, 144)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 32, 32, 24)   31104       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 32, 32, 24)   0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 32, 32, 168)  0           concatenate_76[0][0]             \n",
      "                                                                 dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 32, 32, 168)  672         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 32, 32, 168)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 32, 32, 24)   36288       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 32, 32, 24)   0           conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 32, 32, 192)  0           concatenate_77[0][0]             \n",
      "                                                                 dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 32, 32, 192)  768         concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 32, 32, 192)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 24)   41472       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 32, 32, 24)   0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 32, 32, 216)  0           concatenate_78[0][0]             \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 32, 32, 216)  864         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 216)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 24)   46656       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 32, 32, 24)   0           conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 32, 32, 240)  0           concatenate_79[0][0]             \n",
      "                                                                 dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 32, 32, 240)  960         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 240)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 32, 32, 24)   5760        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 32, 32, 24)   0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 16, 16, 24)   0           dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 16, 16, 24)   96          average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 16, 16, 24)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 24)   5184        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 16, 16, 24)   0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 16, 16, 48)   0           average_pooling2d_8[0][0]        \n",
      "                                                                 dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 16, 16, 48)   192         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 16, 16, 48)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 16, 16, 24)   10368       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 16, 16, 24)   0           conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 16, 16, 72)   0           concatenate_81[0][0]             \n",
      "                                                                 dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 16, 16, 72)   288         concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 16, 16, 72)   0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 16, 16, 24)   15552       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 16, 16, 24)   0           conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 16, 16, 96)   0           concatenate_82[0][0]             \n",
      "                                                                 dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 16, 16, 96)   384         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 16, 16, 96)   0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 16, 16, 24)   20736       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 16, 16, 24)   0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 16, 16, 120)  0           concatenate_83[0][0]             \n",
      "                                                                 dropout_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 16, 16, 120)  480         concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 16, 16, 120)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 16, 16, 24)   25920       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 16, 16, 24)   0           conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 16, 16, 144)  0           concatenate_84[0][0]             \n",
      "                                                                 dropout_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 16, 16, 144)  576         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 144)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 16, 16, 24)   31104       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 16, 16, 24)   0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 16, 16, 168)  0           concatenate_85[0][0]             \n",
      "                                                                 dropout_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 16, 16, 168)  672         concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 168)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 16, 16, 24)   36288       activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 16, 16, 24)   0           conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 16, 16, 192)  0           concatenate_86[0][0]             \n",
      "                                                                 dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 16, 16, 192)  768         concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 192)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 16, 16, 24)   41472       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 16, 16, 24)   0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 16, 16, 216)  0           concatenate_87[0][0]             \n",
      "                                                                 dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 16, 16, 216)  864         concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 16, 16, 216)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 16, 16, 24)   46656       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 16, 16, 24)   0           conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 16, 16, 240)  0           concatenate_88[0][0]             \n",
      "                                                                 dropout_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 16, 16, 240)  960         concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 240)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 16, 16, 24)   5760        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)            (None, 16, 16, 24)   0           conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 24)     0           dropout_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 8, 8, 24)     96          average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 8, 8, 24)     0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 8, 8, 24)     5184        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 8, 8, 24)     0           conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 8, 8, 48)     0           average_pooling2d_9[0][0]        \n",
      "                                                                 dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 8, 8, 48)     192         concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 8, 8, 48)     0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 8, 8, 24)     10368       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 8, 8, 24)     0           conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 8, 8, 72)     0           concatenate_90[0][0]             \n",
      "                                                                 dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 8, 8, 72)     288         concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 8, 8, 72)     0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 8, 8, 24)     15552       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)           (None, 8, 8, 24)     0           conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 8, 8, 96)     0           concatenate_91[0][0]             \n",
      "                                                                 dropout_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 8, 8, 96)     384         concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 8, 8, 96)     0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 8, 8, 24)     20736       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)           (None, 8, 8, 24)     0           conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 8, 8, 120)    0           concatenate_92[0][0]             \n",
      "                                                                 dropout_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 8, 8, 120)    480         concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 8, 8, 120)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 8, 8, 24)     25920       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)           (None, 8, 8, 24)     0           conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 8, 8, 144)    0           concatenate_93[0][0]             \n",
      "                                                                 dropout_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 8, 8, 144)    576         concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 8, 8, 144)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 8, 8, 24)     31104       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)           (None, 8, 8, 24)     0           conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 8, 8, 168)    0           concatenate_94[0][0]             \n",
      "                                                                 dropout_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 8, 8, 168)    672         concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 8, 8, 168)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 8, 8, 24)     36288       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)           (None, 8, 8, 24)     0           conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 8, 8, 192)    0           concatenate_95[0][0]             \n",
      "                                                                 dropout_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 8, 8, 192)    768         concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 8, 8, 192)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 8, 8, 24)     41472       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 8, 8, 24)     0           conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 8, 8, 216)    0           concatenate_96[0][0]             \n",
      "                                                                 dropout_105[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 8, 8, 216)    864         concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 8, 8, 216)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 8, 8, 24)     46656       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)           (None, 8, 8, 24)     0           conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 8, 8, 240)    0           concatenate_97[0][0]             \n",
      "                                                                 dropout_106[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 8, 8, 240)    960         concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 8, 8, 240)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 8, 8, 24)     5760        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)           (None, 8, 8, 24)     0           conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 4, 4, 24)     0           dropout_107[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 4, 4, 24)     96          average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 4, 4, 24)     0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 4, 4, 24)     5184        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)           (None, 4, 4, 24)     0           conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 4, 4, 48)     0           average_pooling2d_10[0][0]       \n",
      "                                                                 dropout_108[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 4, 4, 48)     192         concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 4, 4, 48)     0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 4, 4, 24)     10368       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)           (None, 4, 4, 24)     0           conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 4, 4, 72)     0           concatenate_99[0][0]             \n",
      "                                                                 dropout_109[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 4, 4, 72)     288         concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 4, 4, 72)     0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 4, 4, 24)     15552       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)           (None, 4, 4, 24)     0           conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_101 (Concatenate)   (None, 4, 4, 96)     0           concatenate_100[0][0]            \n",
      "                                                                 dropout_110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 4, 4, 96)     384         concatenate_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 4, 4, 96)     0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 4, 4, 24)     20736       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)           (None, 4, 4, 24)     0           conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_102 (Concatenate)   (None, 4, 4, 120)    0           concatenate_101[0][0]            \n",
      "                                                                 dropout_111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 4, 4, 120)    480         concatenate_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 4, 4, 120)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 4, 4, 24)     25920       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)           (None, 4, 4, 24)     0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_103 (Concatenate)   (None, 4, 4, 144)    0           concatenate_102[0][0]            \n",
      "                                                                 dropout_112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 4, 4, 144)    576         concatenate_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 4, 4, 144)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 4, 4, 24)     31104       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 4, 4, 24)     0           conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 4, 4, 168)    0           concatenate_103[0][0]            \n",
      "                                                                 dropout_113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 4, 4, 168)    672         concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 4, 4, 168)    0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 4, 4, 24)     36288       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)           (None, 4, 4, 24)     0           conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 4, 4, 192)    0           concatenate_104[0][0]            \n",
      "                                                                 dropout_114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 4, 4, 192)    768         concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 4, 4, 192)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 4, 4, 24)     41472       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)           (None, 4, 4, 24)     0           conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 4, 4, 216)    0           concatenate_105[0][0]            \n",
      "                                                                 dropout_115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 4, 4, 216)    864         concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 4, 4, 216)    0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 4, 4, 24)     46656       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 4, 4, 24)     0           conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 4, 4, 240)    0           concatenate_106[0][0]            \n",
      "                                                                 dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 4, 4, 240)    960         concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 4, 4, 240)    0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 2, 2, 240)    0           activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 2, 2, 10)     2400        average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 10)           0           conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 10)           0           global_max_pooling2d_2[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 974,568\n",
      "Trainable params: 964,008\n",
      "Non-trainable params: 10,560\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953/1953 [============================>.] - ETA: 0s - loss: 1.3263 - acc: 0.5146Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 11s 1ms/sample - loss: 2.4117 - acc: 0.4869\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48690, saving model to gdrive/My Drive/cnnoncifar/models/model-001-0.514679-0.486900.h5\n",
      "1954/1953 [==============================] - 948s 485ms/step - loss: 1.3261 - acc: 0.5147 - val_loss: 2.1875 - val_acc: 0.4869\n",
      "Epoch 2/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.9238 - acc: 0.6700Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 873us/sample - loss: 1.3751 - acc: 0.6039\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48690 to 0.60390, saving model to gdrive/My Drive/cnnoncifar/models/model-002-0.670043-0.603900.h5\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.9237 - acc: 0.6700 - val_loss: 1.4341 - val_acc: 0.6039\n",
      "Epoch 3/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.7215Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 867us/sample - loss: 0.6290 - acc: 0.7389\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60390 to 0.73890, saving model to gdrive/My Drive/cnnoncifar/models/model-003-0.721513-0.738900.h5\n",
      "1954/1953 [==============================] - 896s 459ms/step - loss: 0.7871 - acc: 0.7215 - val_loss: 0.8146 - val_acc: 0.7389\n",
      "Epoch 4/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.7034 - acc: 0.7523Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 867us/sample - loss: 0.7341 - acc: 0.6968\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.73890\n",
      "1954/1953 [==============================] - 896s 459ms/step - loss: 0.7034 - acc: 0.7523 - val_loss: 0.9627 - val_acc: 0.6968\n",
      "Epoch 5/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.6418 - acc: 0.7744Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 868us/sample - loss: 0.8542 - acc: 0.7726\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.73890 to 0.77260, saving model to gdrive/My Drive/cnnoncifar/models/model-005-0.774404-0.772600.h5\n",
      "1954/1953 [==============================] - 895s 458ms/step - loss: 0.6418 - acc: 0.7744 - val_loss: 0.7399 - val_acc: 0.7726\n",
      "Epoch 6/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.5994 - acc: 0.7902Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 866us/sample - loss: 0.5927 - acc: 0.7792\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77260 to 0.77920, saving model to gdrive/My Drive/cnnoncifar/models/model-006-0.790261-0.779200.h5\n",
      "1954/1953 [==============================] - 896s 459ms/step - loss: 0.5993 - acc: 0.7903 - val_loss: 0.6867 - val_acc: 0.7792\n",
      "Epoch 7/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8025Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 867us/sample - loss: 0.6857 - acc: 0.7771\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77920\n",
      "1954/1953 [==============================] - 896s 458ms/step - loss: 0.5638 - acc: 0.8025 - val_loss: 0.7391 - val_acc: 0.7771\n",
      "Epoch 8/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.8121Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 870us/sample - loss: 0.5309 - acc: 0.8246\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.77920 to 0.82460, saving model to gdrive/My Drive/cnnoncifar/models/model-008-0.812116-0.824600.h5\n",
      "1954/1953 [==============================] - 898s 460ms/step - loss: 0.5349 - acc: 0.8121 - val_loss: 0.5517 - val_acc: 0.8246\n",
      "Epoch 9/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8211Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 871us/sample - loss: 0.7793 - acc: 0.7970\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82460\n",
      "1954/1953 [==============================] - 896s 459ms/step - loss: 0.5086 - acc: 0.8211 - val_loss: 0.6852 - val_acc: 0.7970\n",
      "Epoch 10/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8287Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 869us/sample - loss: 0.7258 - acc: 0.7877\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82460\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.4893 - acc: 0.8287 - val_loss: 0.7360 - val_acc: 0.7877\n",
      "Epoch 11/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.8345Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 869us/sample - loss: 0.7942 - acc: 0.8304\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.82460 to 0.83040, saving model to gdrive/My Drive/cnnoncifar/models/model-011-0.834511-0.830400.h5\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.4712 - acc: 0.8345 - val_loss: 0.5485 - val_acc: 0.8304\n",
      "Epoch 12/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8402Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 871us/sample - loss: 0.4233 - acc: 0.8440\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.83040 to 0.84400, saving model to gdrive/My Drive/cnnoncifar/models/model-012-0.840162-0.844000.h5\n",
      "1954/1953 [==============================] - 898s 460ms/step - loss: 0.4553 - acc: 0.8402 - val_loss: 0.4925 - val_acc: 0.8440\n",
      "Epoch 13/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.8453Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 870us/sample - loss: 0.5550 - acc: 0.8282\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.84400\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.4421 - acc: 0.8453 - val_loss: 0.5625 - val_acc: 0.8282\n",
      "Epoch 14/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.8501Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 875us/sample - loss: 0.5656 - acc: 0.8378\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84400\n",
      "1954/1953 [==============================] - 894s 457ms/step - loss: 0.4289 - acc: 0.8500 - val_loss: 0.5419 - val_acc: 0.8378\n",
      "Epoch 15/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8542Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 873us/sample - loss: 0.3844 - acc: 0.8407\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84400\n",
      "1954/1953 [==============================] - 896s 458ms/step - loss: 0.4161 - acc: 0.8542 - val_loss: 0.5108 - val_acc: 0.8407\n",
      "Epoch 16/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8585Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 870us/sample - loss: 0.4045 - acc: 0.8563\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84400 to 0.85630, saving model to gdrive/My Drive/cnnoncifar/models/model-016-0.858456-0.856300.h5\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.4061 - acc: 0.8585 - val_loss: 0.4657 - val_acc: 0.8563\n",
      "Epoch 17/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8610Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 868us/sample - loss: 0.4291 - acc: 0.8581\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.85630 to 0.85810, saving model to gdrive/My Drive/cnnoncifar/models/model-017-0.861049-0.858100.h5\n",
      "1954/1953 [==============================] - 896s 459ms/step - loss: 0.3975 - acc: 0.8610 - val_loss: 0.4625 - val_acc: 0.8581\n",
      "Epoch 18/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8636Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 864us/sample - loss: 0.5011 - acc: 0.8249\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85810\n",
      "1954/1953 [==============================] - 895s 458ms/step - loss: 0.3877 - acc: 0.8636 - val_loss: 0.5859 - val_acc: 0.8249\n",
      "Epoch 19/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8669Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 871us/sample - loss: 0.5224 - acc: 0.8575\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85810\n",
      "1954/1953 [==============================] - 893s 457ms/step - loss: 0.3805 - acc: 0.8669 - val_loss: 0.4739 - val_acc: 0.8575\n",
      "Epoch 20/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8694Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 864us/sample - loss: 0.5189 - acc: 0.8610\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.85810 to 0.86100, saving model to gdrive/My Drive/cnnoncifar/models/model-020-0.869445-0.861000.h5\n",
      "1954/1953 [==============================] - 896s 458ms/step - loss: 0.3712 - acc: 0.8694 - val_loss: 0.4580 - val_acc: 0.8610\n",
      "Epoch 21/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8718Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 869us/sample - loss: 0.5567 - acc: 0.8217\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86100\n",
      "1954/1953 [==============================] - 896s 458ms/step - loss: 0.3650 - acc: 0.8718 - val_loss: 0.6434 - val_acc: 0.8217\n",
      "Epoch 22/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8743Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 869us/sample - loss: 0.3746 - acc: 0.8685\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.86100 to 0.86850, saving model to gdrive/My Drive/cnnoncifar/models/model-022-0.874284-0.868500.h5\n",
      "1954/1953 [==============================] - 900s 461ms/step - loss: 0.3581 - acc: 0.8743 - val_loss: 0.4142 - val_acc: 0.8685\n",
      "Epoch 23/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8769Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 868us/sample - loss: 0.5342 - acc: 0.8205\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86850\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.3504 - acc: 0.8769 - val_loss: 0.5972 - val_acc: 0.8205\n",
      "Epoch 24/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8796Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 872us/sample - loss: 0.3367 - acc: 0.8749\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.86850 to 0.87490, saving model to gdrive/My Drive/cnnoncifar/models/model-024-0.879570-0.874900.h5\n",
      "1954/1953 [==============================] - 897s 459ms/step - loss: 0.3437 - acc: 0.8796 - val_loss: 0.4168 - val_acc: 0.8749\n",
      "Epoch 25/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8805Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 873us/sample - loss: 0.3277 - acc: 0.8692\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87490\n",
      "1954/1953 [==============================] - 899s 460ms/step - loss: 0.3399 - acc: 0.8805 - val_loss: 0.4483 - val_acc: 0.8692\n",
      "Epoch 26/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8827Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 868us/sample - loss: 0.4074 - acc: 0.8695\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87490\n",
      "1954/1953 [==============================] - 896s 459ms/step - loss: 0.3329 - acc: 0.8827 - val_loss: 0.4386 - val_acc: 0.8695\n",
      "Epoch 27/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8841Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 866us/sample - loss: 0.2969 - acc: 0.8739\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87490\n",
      "1954/1953 [==============================] - 895s 458ms/step - loss: 0.3300 - acc: 0.8841 - val_loss: 0.4191 - val_acc: 0.8739\n",
      "Epoch 28/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8863Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 870us/sample - loss: 0.3948 - acc: 0.8735\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87490\n",
      "1954/1953 [==============================] - 896s 458ms/step - loss: 0.3246 - acc: 0.8863 - val_loss: 0.4408 - val_acc: 0.8735\n",
      "Epoch 29/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8871Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 866us/sample - loss: 0.4339 - acc: 0.8747\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87490\n",
      "1954/1953 [==============================] - 896s 458ms/step - loss: 0.3203 - acc: 0.8871 - val_loss: 0.4207 - val_acc: 0.8747\n",
      "Epoch 30/150\n",
      "1693/1953 [=========================>....] - ETA: 1:58 - loss: 0.3151 - acc: 0.8887Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "model = dense_net2(X_train,X_test,epoch=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eL6ip1VV8Yo"
   },
   "source": [
    "<strong> At epoch 33 the accuracy was 87% but because of time restrictions by google colab fitting the model is stoped any how since model is already saved at 33rd epoch we shall continue to fit from that epoch</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "Tifc_AsqVY_O",
    "outputId": "c61dbcdf-4a57-4600-dbab-bf16628da656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9161Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 8s 848us/sample - loss: 0.3856 - acc: 0.8887\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90130\n",
      "1954/1953 [==============================] - 886s 454ms/step - loss: 0.2363 - acc: 0.9161 - val_loss: 0.4000 - val_acc: 0.8887\n",
      "Epoch 62/150\n",
      "1699/1953 [=========================>....] - ETA: 1:53 - loss: 0.2356 - acc: 0.9166Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 5, min_lr = 0.000001)\n",
    "\n",
    "# early_stop = EarlyStopping(monitor = \"val_loss\", patience = 10)\n",
    "\n",
    "def decay_fn(epoch, lr):\n",
    "    if epoch < 50:\n",
    "        return 0.001\n",
    "    elif epoch >= 50 and epoch < 75:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(decay_fn)\n",
    "\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('gdrive/My Drive/cnnoncifar/models/model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5',\n",
    "                                       verbose=1, monitor='val_acc',save_best_only=True, mode='auto')  \n",
    "\n",
    "         \n",
    "model.load_weights('gdrive/My Drive/cnnoncifar/models/model-033-0.893153-0.879100.h5')\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(xtrain, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1, \n",
    "#                     validation_data=(xtest, y_test))\n",
    "print(model.summary())\n",
    "model.fit_generator(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size), \n",
    "    steps_per_epoch=(len(X_train)/batch_size)*5,\n",
    "              \n",
    "    epochs=150, verbose = 1,initial_epoch = 32, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrMUjiG4Wz-H"
   },
   "source": [
    "<strong> At epoch 60 the accuracy was 90% but because of time restrictions by google colab fitting the model is stoped any how since model is already saved at 60rd epoch we shall continue to fit from that epoch</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zRSKA6kioM1d",
    "outputId": "de18c3eb-b176-4b94-c406-59d4d437e438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 32, 32, 24)   648         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 32, 32, 24)   96          conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 32, 32, 24)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 32, 32, 24)   5184        activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_163 (Dropout)           (None, 32, 32, 24)   0           conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_160 (Concatenate)   (None, 32, 32, 48)   0           conv2d_165[0][0]                 \n",
      "                                                                 dropout_163[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 32, 32, 48)   192         concatenate_160[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 32, 32, 48)   0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 32, 32, 24)   10368       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_164 (Dropout)           (None, 32, 32, 24)   0           conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_161 (Concatenate)   (None, 32, 32, 72)   0           concatenate_160[0][0]            \n",
      "                                                                 dropout_164[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 32, 32, 72)   288         concatenate_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 32, 32, 72)   0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 32, 32, 24)   15552       activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_165 (Dropout)           (None, 32, 32, 24)   0           conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_162 (Concatenate)   (None, 32, 32, 96)   0           concatenate_161[0][0]            \n",
      "                                                                 dropout_165[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 32, 32, 96)   384         concatenate_162[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 32, 32, 96)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 32, 32, 24)   20736       activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_166 (Dropout)           (None, 32, 32, 24)   0           conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_163 (Concatenate)   (None, 32, 32, 120)  0           concatenate_162[0][0]            \n",
      "                                                                 dropout_166[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 32, 32, 120)  480         concatenate_163[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 32, 32, 120)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 32, 32, 24)   25920       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_167 (Dropout)           (None, 32, 32, 24)   0           conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_164 (Concatenate)   (None, 32, 32, 144)  0           concatenate_163[0][0]            \n",
      "                                                                 dropout_167[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 32, 32, 144)  576         concatenate_164[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 32, 32, 144)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 32, 32, 24)   31104       activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_168 (Dropout)           (None, 32, 32, 24)   0           conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_165 (Concatenate)   (None, 32, 32, 168)  0           concatenate_164[0][0]            \n",
      "                                                                 dropout_168[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 32, 32, 168)  672         concatenate_165[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 32, 32, 168)  0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 32, 32, 24)   36288       activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_169 (Dropout)           (None, 32, 32, 24)   0           conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_166 (Concatenate)   (None, 32, 32, 192)  0           concatenate_165[0][0]            \n",
      "                                                                 dropout_169[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 32, 32, 192)  768         concatenate_166[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 32, 32, 192)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 32, 32, 24)   41472       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_170 (Dropout)           (None, 32, 32, 24)   0           conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_167 (Concatenate)   (None, 32, 32, 216)  0           concatenate_166[0][0]            \n",
      "                                                                 dropout_170[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 32, 32, 216)  864         concatenate_167[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 32, 32, 216)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 32, 32, 24)   46656       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_171 (Dropout)           (None, 32, 32, 24)   0           conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_168 (Concatenate)   (None, 32, 32, 240)  0           concatenate_167[0][0]            \n",
      "                                                                 dropout_171[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 32, 32, 240)  960         concatenate_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 32, 32, 240)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 32, 32, 24)   5760        activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_172 (Dropout)           (None, 32, 32, 24)   0           conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 16, 16, 24)   0           dropout_172[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 16, 16, 24)   96          average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 16, 16, 24)   0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 16, 16, 24)   5184        activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_173 (Dropout)           (None, 16, 16, 24)   0           conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_169 (Concatenate)   (None, 16, 16, 48)   0           average_pooling2d_4[0][0]        \n",
      "                                                                 dropout_173[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 16, 16, 48)   192         concatenate_169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 16, 16, 48)   0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 16, 16, 24)   10368       activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_174 (Dropout)           (None, 16, 16, 24)   0           conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_170 (Concatenate)   (None, 16, 16, 72)   0           concatenate_169[0][0]            \n",
      "                                                                 dropout_174[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 16, 16, 72)   288         concatenate_170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 16, 16, 72)   0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 16, 16, 24)   15552       activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_175 (Dropout)           (None, 16, 16, 24)   0           conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_171 (Concatenate)   (None, 16, 16, 96)   0           concatenate_170[0][0]            \n",
      "                                                                 dropout_175[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 16, 16, 96)   384         concatenate_171[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 16, 16, 96)   0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 16, 16, 24)   20736       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_176 (Dropout)           (None, 16, 16, 24)   0           conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_172 (Concatenate)   (None, 16, 16, 120)  0           concatenate_171[0][0]            \n",
      "                                                                 dropout_176[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 16, 16, 120)  480         concatenate_172[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 16, 16, 120)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 16, 16, 24)   25920       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_177 (Dropout)           (None, 16, 16, 24)   0           conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_173 (Concatenate)   (None, 16, 16, 144)  0           concatenate_172[0][0]            \n",
      "                                                                 dropout_177[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 16, 16, 144)  576         concatenate_173[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 16, 16, 144)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 16, 16, 24)   31104       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_178 (Dropout)           (None, 16, 16, 24)   0           conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_174 (Concatenate)   (None, 16, 16, 168)  0           concatenate_173[0][0]            \n",
      "                                                                 dropout_178[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 16, 16, 168)  672         concatenate_174[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 16, 16, 168)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 16, 16, 24)   36288       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_179 (Dropout)           (None, 16, 16, 24)   0           conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_175 (Concatenate)   (None, 16, 16, 192)  0           concatenate_174[0][0]            \n",
      "                                                                 dropout_179[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 16, 16, 192)  768         concatenate_175[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 16, 16, 192)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 16, 16, 24)   41472       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_180 (Dropout)           (None, 16, 16, 24)   0           conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_176 (Concatenate)   (None, 16, 16, 216)  0           concatenate_175[0][0]            \n",
      "                                                                 dropout_180[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 16, 16, 216)  864         concatenate_176[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 16, 16, 216)  0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 16, 16, 24)   46656       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_181 (Dropout)           (None, 16, 16, 24)   0           conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_177 (Concatenate)   (None, 16, 16, 240)  0           concatenate_176[0][0]            \n",
      "                                                                 dropout_181[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 16, 16, 240)  960         concatenate_177[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 16, 16, 240)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 16, 16, 24)   5760        activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_182 (Dropout)           (None, 16, 16, 24)   0           conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 8, 8, 24)     0           dropout_182[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 8, 8, 24)     96          average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 8, 8, 24)     0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 8, 8, 24)     5184        activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_183 (Dropout)           (None, 8, 8, 24)     0           conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_178 (Concatenate)   (None, 8, 8, 48)     0           average_pooling2d_5[0][0]        \n",
      "                                                                 dropout_183[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 8, 8, 48)     192         concatenate_178[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 8, 8, 48)     0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 8, 8, 24)     10368       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_184 (Dropout)           (None, 8, 8, 24)     0           conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_179 (Concatenate)   (None, 8, 8, 72)     0           concatenate_178[0][0]            \n",
      "                                                                 dropout_184[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 8, 8, 72)     288         concatenate_179[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 8, 8, 72)     0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 8, 8, 24)     15552       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_185 (Dropout)           (None, 8, 8, 24)     0           conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_180 (Concatenate)   (None, 8, 8, 96)     0           concatenate_179[0][0]            \n",
      "                                                                 dropout_185[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 8, 8, 96)     384         concatenate_180[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 8, 8, 96)     0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 8, 8, 24)     20736       activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_186 (Dropout)           (None, 8, 8, 24)     0           conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_181 (Concatenate)   (None, 8, 8, 120)    0           concatenate_180[0][0]            \n",
      "                                                                 dropout_186[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 8, 8, 120)    480         concatenate_181[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 8, 8, 120)    0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 8, 8, 24)     25920       activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_187 (Dropout)           (None, 8, 8, 24)     0           conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_182 (Concatenate)   (None, 8, 8, 144)    0           concatenate_181[0][0]            \n",
      "                                                                 dropout_187[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 8, 8, 144)    576         concatenate_182[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 8, 8, 144)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 8, 8, 24)     31104       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_188 (Dropout)           (None, 8, 8, 24)     0           conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_183 (Concatenate)   (None, 8, 8, 168)    0           concatenate_182[0][0]            \n",
      "                                                                 dropout_188[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 8, 8, 168)    672         concatenate_183[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 8, 8, 168)    0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 8, 8, 24)     36288       activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_189 (Dropout)           (None, 8, 8, 24)     0           conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_184 (Concatenate)   (None, 8, 8, 192)    0           concatenate_183[0][0]            \n",
      "                                                                 dropout_189[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 8, 8, 192)    768         concatenate_184[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 8, 8, 192)    0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 8, 8, 24)     41472       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_190 (Dropout)           (None, 8, 8, 24)     0           conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_185 (Concatenate)   (None, 8, 8, 216)    0           concatenate_184[0][0]            \n",
      "                                                                 dropout_190[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 8, 8, 216)    864         concatenate_185[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 8, 8, 216)    0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 8, 8, 24)     46656       activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_191 (Dropout)           (None, 8, 8, 24)     0           conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_186 (Concatenate)   (None, 8, 8, 240)    0           concatenate_185[0][0]            \n",
      "                                                                 dropout_191[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 8, 8, 240)    960         concatenate_186[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 8, 8, 240)    0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 8, 8, 24)     5760        activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_192 (Dropout)           (None, 8, 8, 24)     0           conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 4, 4, 24)     0           dropout_192[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 4, 4, 24)     96          average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 4, 4, 24)     0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 4, 4, 24)     5184        activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_193 (Dropout)           (None, 4, 4, 24)     0           conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_187 (Concatenate)   (None, 4, 4, 48)     0           average_pooling2d_6[0][0]        \n",
      "                                                                 dropout_193[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 4, 4, 48)     192         concatenate_187[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 4, 4, 48)     0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 4, 4, 24)     10368       activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_194 (Dropout)           (None, 4, 4, 24)     0           conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_188 (Concatenate)   (None, 4, 4, 72)     0           concatenate_187[0][0]            \n",
      "                                                                 dropout_194[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 4, 4, 72)     288         concatenate_188[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 4, 4, 72)     0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 4, 4, 24)     15552       activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_195 (Dropout)           (None, 4, 4, 24)     0           conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_189 (Concatenate)   (None, 4, 4, 96)     0           concatenate_188[0][0]            \n",
      "                                                                 dropout_195[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 4, 4, 96)     384         concatenate_189[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 4, 4, 96)     0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 4, 4, 24)     20736       activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_196 (Dropout)           (None, 4, 4, 24)     0           conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_190 (Concatenate)   (None, 4, 4, 120)    0           concatenate_189[0][0]            \n",
      "                                                                 dropout_196[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 4, 4, 120)    480         concatenate_190[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 4, 4, 120)    0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 4, 4, 24)     25920       activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_197 (Dropout)           (None, 4, 4, 24)     0           conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_191 (Concatenate)   (None, 4, 4, 144)    0           concatenate_190[0][0]            \n",
      "                                                                 dropout_197[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 4, 4, 144)    576         concatenate_191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 4, 4, 144)    0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 4, 4, 24)     31104       activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_198 (Dropout)           (None, 4, 4, 24)     0           conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_192 (Concatenate)   (None, 4, 4, 168)    0           concatenate_191[0][0]            \n",
      "                                                                 dropout_198[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 4, 4, 168)    672         concatenate_192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 4, 4, 168)    0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 4, 4, 24)     36288       activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_199 (Dropout)           (None, 4, 4, 24)     0           conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_193 (Concatenate)   (None, 4, 4, 192)    0           concatenate_192[0][0]            \n",
      "                                                                 dropout_199[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 4, 4, 192)    768         concatenate_193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 4, 4, 192)    0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 4, 4, 24)     41472       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_200 (Dropout)           (None, 4, 4, 24)     0           conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_194 (Concatenate)   (None, 4, 4, 216)    0           concatenate_193[0][0]            \n",
      "                                                                 dropout_200[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 4, 4, 216)    864         concatenate_194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 4, 4, 216)    0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 4, 4, 24)     46656       activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_201 (Dropout)           (None, 4, 4, 24)     0           conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_195 (Concatenate)   (None, 4, 4, 240)    0           concatenate_194[0][0]            \n",
      "                                                                 dropout_201[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 4, 4, 240)    960         concatenate_195[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 4, 4, 240)    0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 2, 2, 240)    0           activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 2, 2, 10)     2400        average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 10)           0           conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 10)           0           global_max_pooling2d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 974,568\n",
      "Trainable params: 964,008\n",
      "Non-trainable params: 10,560\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9162Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 12s 1ms/sample - loss: 0.2617 - acc: 0.9017\n",
      "\n",
      "Epoch 00062: val_acc improved from -inf to 0.90170, saving model to gdrive/My Drive/cnnoncifar/models/model-062-0.916165-0.901700.h5\n",
      "1954/1953 [==============================] - 963s 493ms/step - loss: 0.2368 - acc: 0.9162 - val_loss: 0.3534 - val_acc: 0.9017\n",
      "Epoch 63/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9157Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 867us/sample - loss: 0.3246 - acc: 0.8949\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 891s 456ms/step - loss: 0.2354 - acc: 0.9156 - val_loss: 0.3893 - val_acc: 0.8949\n",
      "Epoch 64/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9175Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 862us/sample - loss: 0.2827 - acc: 0.8848\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 892s 456ms/step - loss: 0.2332 - acc: 0.9175 - val_loss: 0.4301 - val_acc: 0.8848\n",
      "Epoch 65/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9183Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 861us/sample - loss: 0.2895 - acc: 0.8879\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 893s 457ms/step - loss: 0.2312 - acc: 0.9183 - val_loss: 0.4231 - val_acc: 0.8879\n",
      "Epoch 66/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9191Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 855us/sample - loss: 0.2836 - acc: 0.8820\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 882s 452ms/step - loss: 0.2289 - acc: 0.9191 - val_loss: 0.4103 - val_acc: 0.8820\n",
      "Epoch 67/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9183Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 851us/sample - loss: 0.4144 - acc: 0.8848\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 877s 449ms/step - loss: 0.2299 - acc: 0.9183 - val_loss: 0.4098 - val_acc: 0.8848\n",
      "Epoch 68/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9182Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 852us/sample - loss: 0.2453 - acc: 0.8931\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 876s 448ms/step - loss: 0.2286 - acc: 0.9182 - val_loss: 0.3699 - val_acc: 0.8931\n",
      "Epoch 69/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9190Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 852us/sample - loss: 0.3324 - acc: 0.8846\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 875s 448ms/step - loss: 0.2286 - acc: 0.9190 - val_loss: 0.4226 - val_acc: 0.8846\n",
      "Epoch 70/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9204Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 859us/sample - loss: 0.3675 - acc: 0.8863\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90170\n",
      "1954/1953 [==============================] - 877s 449ms/step - loss: 0.2242 - acc: 0.9204 - val_loss: 0.4263 - val_acc: 0.8863\n",
      "Epoch 71/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9207Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 858us/sample - loss: 0.2657 - acc: 0.9024\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.90170 to 0.90240, saving model to gdrive/My Drive/cnnoncifar/models/model-071-0.920683-0.902400.h5\n",
      "1954/1953 [==============================] - 880s 450ms/step - loss: 0.2253 - acc: 0.9207 - val_loss: 0.3447 - val_acc: 0.9024\n",
      "Epoch 72/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9211Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 857us/sample - loss: 0.3801 - acc: 0.8939\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90240\n",
      "1954/1953 [==============================] - 881s 451ms/step - loss: 0.2222 - acc: 0.9211 - val_loss: 0.3733 - val_acc: 0.8939\n",
      "Epoch 73/150\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9212Epoch 1/150\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 872us/sample - loss: 0.3135 - acc: 0.8923\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90240\n",
      "1954/1953 [==============================] - 886s 454ms/step - loss: 0.2211 - acc: 0.9212 - val_loss: 0.3861 - val_acc: 0.8923\n",
      "Epoch 74/150\n",
      " 654/1953 [=========>....................] - ETA: 9:47 - loss: 0.2212 - acc: 0.9207"
     ]
    }
   ],
   "source": [
    "# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 5, min_lr = 0.000001)\n",
    "\n",
    "# early_stop = EarlyStopping(monitor = \"val_loss\", patience = 10)\n",
    "\n",
    "def decay_fn(epoch, lr):\n",
    "    if epoch < 50:\n",
    "        return 0.001\n",
    "    elif epoch >= 50 and epoch < 75:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(decay_fn)\n",
    "\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('gdrive/My Drive/cnnoncifar/models/model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5',\n",
    "                                       verbose=1, monitor='val_acc',save_best_only=True, mode='auto')  \n",
    "\n",
    "         \n",
    "model.load_weights('gdrive/My Drive/cnnoncifar/models/model-060-0.915397-0.901300.h5')\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(xtrain, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1, \n",
    "#                     validation_data=(xtest, y_test))\n",
    "print(model.summary())\n",
    "model.fit_generator(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size), \n",
    "    steps_per_epoch=(len(X_train)/batch_size)*5,\n",
    "              \n",
    "    epochs=150, verbose = 1,initial_epoch = 61, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gXHporac0JfG",
    "outputId": "4748dab1-714c-4bfd-c2ff-deb3aff9c473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 60s 1ms/sample - loss: 0.1094 - acc: 0.9612\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.3466 - acc: 0.9024\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('gdrive/My Drive/cnnoncifar/models/model-071-0.920683-0.902400.h5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "\n",
    "train_acc = model.evaluate(X_train,y_train)\n",
    "val_acc   = model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qaNRw5UkeAdn",
    "outputId": "efe1e34b-50dc-43ff-95c1-afc10dd03fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96122 0.9024\n"
     ]
    }
   ],
   "source": [
    "print(train_acc[1],val_acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Ij6vQW--nSsh",
    "outputId": "7be73920-7f8d-4b2d-d47b-cfb990e269fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train accuracy is     : 96%\n",
      "The test accuracy is      : 90.24 i.e ~91%\n",
      "Number of parameters used : 974568\n"
     ]
    }
   ],
   "source": [
    "print('The train accuracy is     : {}%'.format(96))\n",
    "print('The test accuracy is      : {} i.e ~{}%'.format(90.24,91))\n",
    "print('Number of parameters used : {}'.format(model.count_params()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2fLA3SqPZKQz",
    "5UYh4fu4Flu4",
    "3gCJagB5GYRN"
   ],
   "name": "DenseNet_cifar10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
